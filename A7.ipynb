{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7: Training Distillation vs LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\st124\\OneDrive\\Desktop\\NLP_A7\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.4.1', '4.49.0', '2.5.1+cu121')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets --upgrade\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "datasets.__version__, transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load HateXplain Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\st124\\OneDrive\\Desktop\\NLP_A7\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\st124\\.cache\\huggingface\\hub\\datasets--hate_speech_offensive. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 24783/24783 [00:00<00:00, 461769.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the HateXplain dataset from Hugging Face\n",
    "dataset = load_dataset(\"hate_speech_offensive\")\n",
    "\n",
    "# # Check dataset structure\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Mapping\n",
    "label_list = [\"Non-Hate\", \"Offensive\", \"Hate\"]\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "id2label = {i: v for v, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign feature key\n",
    "task_to_keys = {\"hatexplain\": \"tweet\"}\n",
    "task_name = \"hatexplain\"\n",
    "sentence_key = task_to_keys[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'class', 'tweet'],\n",
      "        num_rows: 24783\n",
      "    })\n",
      "})\n",
      "Example: !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\n",
      "Label2ID: {'Non-Hate': 0, 'Offensive': 1, 'Hate': 2}\n",
      "ID2Label: {0: 'Non-Hate', 1: 'Offensive', 2: 'Hate'}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset overview\n",
    "print(dataset)\n",
    "print(\"Example:\", dataset[\"train\"][0][sentence_key])\n",
    "print(\"Label2ID:\", label2id)\n",
    "print(\"ID2Label:\", id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of unique labels\n",
    "num_labels = len(label_list)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\st124\\OneDrive\\Desktop\\NLP_A7\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\st124\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "teacher_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Teacher Model (BERT)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Function (Modified for HateXplain)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tweet\"], max_length=128, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 24783/24783 [00:02<00:00, 10907.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,   999,   999,   999, 19387,  1030,  9815, 19454, 21818,  2135,\n",
      "         1024,  2004,  1037,  2450,  2017,  5807,  1005,  1056, 17612,  2055,\n",
      "         9344,  2039,  2115,  2160,  1012,  1004, 23713,  1025,  2004,  1037,\n",
      "         2158,  2017,  2323,  2467,  2202,  1996, 11669,  2041,  1012,  1012,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "[CLS]!!! rt @ mayasolovely : as a woman you shouldn ' t complain about cleaning up your house. & amp ; as a man you should always take the trash out... [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"count\", \"hate_speech_count\", \"offensive_language_count\", \"neither_count\", \"tweet\"])\n",
    "\n",
    "# Rename \"class\" column to \"labels\" for PyTorch compatibility\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"class\", \"labels\")\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Print an example tokenized input\n",
    "print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Collator (Handles Dynamic Padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataset for efficiency\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(10000))  # 10K samples\n",
    "small_eval_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(1000))    # 1K samples (same train split)\n",
    "small_test_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(1000))    # 1K samples (same train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    small_train_dataset, shuffle=True, batch_size=32, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    small_test_dataset, batch_size=32, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    small_eval_dataset, batch_size=32, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.Size([32, 128]), torch.Size([32, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first batch\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "batch['labels'].shape, batch['input_ids'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training for Even and Odd Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
